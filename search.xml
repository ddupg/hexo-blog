<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HBase: SCP &amp; TRSP</title>
    <url>/blog/2020/04/15/SCP&amp;TRSP/</url>
    <content><![CDATA[<p>最近在组内进行的一次SCP和TRSP两个Procedure执行过程的分享，在这里记录一下，懒得去整理出文章了 😜</p>
<a id="more"></a>
<h2 id="AMv2">AMv2</h2>
<h3 id="比较重要的类">比较重要的类</h3>
<p><img src="amv2.png" alt="AMv2主要的类"></p>
<h4 id="AssignmentManager">AssignmentManager</h4>
<p>管理Region的assign/unassign操作，管理Region的状态信息。</p>
<p>其中主要属性：</p>
<ul>
<li>RegionStates: 管理内存中的Region状态信息，RS上有哪些region</li>
<li>RegionStateStore: 操作meta表</li>
</ul>
<h4 id="RegionStateNode">RegionStateNode</h4>
<ul>
<li>lock: 对Region状态信息和RegionLoacation信息加锁，防止并发修改。</li>
<li>regionInfo: Region信息</li>
<li>event: 用于多个procedure等待Region的某个状态变化，比如open、close。</li>
<li>procedure: 记录唯一绑定的TRSP，保证每个Region只能同时进行一个TRSP。</li>
<li>regionLocation: region要被assign到哪个RS。</li>
</ul>
<h4 id="ProcedureEvent">ProcedureEvent</h4>
<p>用于多个Procedure等待某个依赖的事件，在AMv2中，主要的就是等待Region的状态变化。</p>
<p><img src="ProcedureEvent.png" alt="ProcedureEvent"></p>
<p>事件状态就两种：</p>
<ul>
<li>ready: 某个事件已准备好，可以继续执行</li>
<li>suspend: 事件未准备好，之后调用suspendIfNotReady()方法的Procedure都会被加到 suspendedProcedure队列里，用于之后唤醒。</li>
</ul>
<p>主要方法：</p>
<ul>
<li>suspendIfNotReady: 如果是ready状态，表示可以继续执行；如果suspend状态，Procedure会被加到 suspendedProcedure队列里，用于之后唤醒。这时候Procedure会抛出ProcedureSuspendedException，Pv2框架会暂停Procedure的执行，等待被唤醒。</li>
</ul>
<p>这个功能有个问题就是只保存在内存中，无法恢复，如果Procedure使用不好，Master重启，等待队列无法恢复，Procedure可能永远无法被唤醒。</p>
<h2 id="ServerCrashProcedure">ServerCrashProcedure</h2>
<h3 id="触发条件">触发条件</h3>
<h4 id="zk-session-expire（被动触发）">zk session expire（被动触发）</h4>
<p>监听zk上rs节点的变化，如果代表某个rs的子节点被过期删除，就触发SCP。</p>
<p>配置: {zookeeper.znode.parent}/{<a href="http://zookeeper.znode.rs" target="_blank" rel="noopener">zookeeper.znode.rs</a>}/</p>
<p>默认: /hbase/{cluster name}/rs/</p>
<p>此种情况下，如果rs处于非ONLINE状态，不会强制执行ServerCrashProcedure</p>
<h4 id="HBCK2-（主动触发）">HBCK2 （主动触发）</h4>
<p>要使用完整的ServerName，包含后面的startcode。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;hbase hbck -j hbase-hbck2-1.0.0-SNAPSHOT.jar scheduleRecoveries c4-hadoop-tst-st84.bj,55600,1586416554312 c4-hadoop-tst-st85.bj,55600,1586415546993</span><br></pre></td></tr></table></figure>
<p>无论rs是否处于ONLINE状态，都会强制执行 HBCKServerCrashProcedure（ServerCrashProcedure的子类），大多数情况下行为和ServerCrashProcedure一样，不同的地方在于getRegionsOnCrashedServer方法：</p>
<p>如果ServerCrashProcedure.getRegionsOnCrashedServer返回空集合，HBCKServerCarshProcedure会scan读meta表，将meta表上记录的Opening和Opened两种状态的region返回，另外将Closing状态的region改为Close。</p>
<h3 id="状态变化">状态变化</h3>
<p><strong>准备工作</strong></p>
<p>将当前处理的rs加到 DeadServer processing list。在SCP执行结束之后，才加到 DeadServers list。每个状态都会检查下，是否加进去了。</p>
<p><strong>等待meta表可用</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">case SERVER_CRASH_START:</span><br><span class="line">case SERVER_CRASH_SPLIT_META_LOGS:</span><br><span class="line">case SERVER_CRASH_DELETE_SPLIT_META_WALS_DIR:</span><br><span class="line">case SERVER_CRASH_ASSIGN_META:</span><br><span class="line">break;</span><br><span class="line">default:</span><br><span class="line">&#x2F;&#x2F; If hbase:meta is not assigned, yield.</span><br><span class="line">if (env.getAssignmentManager().waitMetaLoaded(this)) &#123;</span><br><span class="line">    throw new ProcedureSuspendedException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>前面几个状态都是操作meta表的region的状态，所以meta表不可用也会执行，其他的状态会操作普通region，涉及到meta表的读写，所以其他状态都要求meta表的region可用，才可以继续执行。这里就利用了ProcedureEvent这个类的功能，等待meta region加载完成之后才允许继续下面的状态，否则直接抛出ProcedureSuspendedException暂停当前Procedure的执行，等待被唤醒。</p>
<p><strong>但这里好像使用AssignmentManager.metaAssignEvent更合适</strong>。因为meatLoadedEvent只有在Master启动之后才会触发一次，metaAssignEvent在Master启动和每次meta region open都会触发，所以metaAssignEvent来代表meta region可用更合适一些。</p>
<p><img src="SCP.png" alt="SCP流程图"></p>
<ul>
<li>START: 没有什么实际操作，根据RS上是否有meta表的region，来判断下一步的状态</li>
<li>SPLIT_META_LOGS: split meta表的WAL</li>
<li>DELETE_SPLIT_META_WALS_DIR: 删除HDFS上meta表split log的目录</li>
<li>ASSIGN_META: 重新assign meta region</li>
<li>GET_REGIONS: 查询RS上除meta外的region</li>
<li>SPLIT_LOGS: split非meta表的WAL</li>
<li>DELETE_SPLIT_WALS_DIR: 删除HDFS上非meta表的WAL目录</li>
<li>ASSIGN: assign 非meta的region</li>
<li>FINISH: 收尾工作，将RS加到 DeadServers list，触发CP</li>
</ul>
<p>其实这些状态主要可以做的就是Split WAL，Delete WAL dir，Assign region这三类，只不过将Region按是否是meta做了区分，meta region和普通region分别做了这三类操作。这里我理解原因应该是普通的region assign需要读写meta表，所以要求meta表的region必须可用才行，所以优先对meta region进行三步操作处理，meta region可用之后才会进行普通region的三步操作。</p>
<h3 id="问题">问题</h3>
<p><strong>执行SCP的时候，RS上的region是怎么处理的？</strong></p>
<p>直接置为ABNORMALLY_CLOSED，认为所有的region都已经不可用了。</p>
<p>如果region有TRSP在进行，就进一步通知所有这些region的TRSP和RRP，做对应的操作。<br>
如果没有，就新加TRSP去assign这些region。</p>
<p>如果rs再恢复或启动的话，也应该会发现zk的节点丢了，不能直接open它的region。（这个没有去看代码确认）</p>
<h2 id="TransitRegionStateProcedure">TransitRegionStateProcedure</h2>
<h3 id="主要变量">主要变量</h3>
<p><strong>type/initialState/lastState</strong></p>
<table>
<thead>
<tr>
<th>type</th>
<th>initialState</th>
<th>lastState</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSIGN</td>
<td>GET_ASSIGN_CANDIDATE</td>
<td>CONFIRM_OPENED</td>
</tr>
<tr>
<td>UNASSIGN</td>
<td>CLOSE</td>
<td>CONFIRM_CLOSED</td>
</tr>
<tr>
<td>MOVE</td>
<td>CLOSE</td>
<td>CONFIRM_OPENED</td>
</tr>
<tr>
<td>REOPEN</td>
<td>CLOSE</td>
<td>CONFIRM_OPENED</td>
</tr>
</tbody>
</table>
<p><strong>remoteProc</strong></p>
<p>绑定的某个RegionRemoteProcedureBase，用于执行open/close region这样的RPC操作，这里也只能有一个remoteProc，表示不能同时进行多个操作region状态的RPC。</p>
<h3 id="状态流转">状态流转</h3>
<p><img src="TRSP-simple.png" alt="TRSP状态简图"></p>
<p>可以看出来，TRSP中的5个状态形成一个环，通过initialState和lastState两个状态判断入环和出环的状态。</p>
<p>像ASSIGN和UNASSIGN两个操作，只需要跑完自己的状态就行了。MOVE/REOPEN操作则要先close然后open，不同的是，通过指定RegionStateNode.regionLocation来指定open在哪个RS上，open的rs变化了就是MOVE，不变就是REOPEN。</p>
<p>ASSIGN: GET_ASSIGN_CANDIDATE -&gt; OPEN -&gt; CONFIRM_OPENED<br>
UNASSIGN: CLOSE -&gt; CONFIRM_CLOSED<br>
MOVE/REOPEN: CLOSE -&gt; CONFIRM_CLOSE -&gt; GET_ASSIGN_CANDIDATE -&gt; OPEN -&gt; CONFIRM_OPENED</p>
<p>简图中一些异常情况下的状态流转没有展示出来，比如如果close失败，会再将region open，然后再close，就会在环上转圈。就是说执行过程中出现问题，就会在环上循环执行，直到满足条件达到lastState最终出环。</p>
<p><img src="TRSP.png" alt="TRSP状态图"></p>
<p><strong>准备工作</strong></p>
<p>执行之前必须获取RegionStateNode的锁，因为执行过程中，会多次对其中的数据（state、regionLocation）做修改，前面讲过，为保证RegionStateNode的属性不会并发修改，所有修改之前都要先拿锁。</p>
<p><strong>GET_ASSIGN_CANDIDATE</strong></p>
<p>主要做的事情就是为Region指定一个RS，将Region放入AM的pendingAssignQueue之后，直接suspend等待。AM以生产者消费者模式为Region指定RS，然后唤醒Procedure。</p>
<p><strong>OPEN</strong></p>
<ol>
<li>如果没有指定regionLocation，则有可能是上一步GET_ASSIGN_CANDIDATE出现问题。或者上一步执行过程中，Master重启，Procedure重试执行了。这时候重新执行上一步就好了。</li>
<li>如果成功制定了regionLocation，就通过OpenRegionProcedure去通知RS open该region。
<ol>
<li>如果OpenRegionProcedure最终成功执行完了，TRSP继续执行</li>
<li>如果失败了，会在下一步CONFIRM_OPENED进行重试</li>
<li>如果Master重启，导致无法唤醒，OpenRegionProcedure有超时机制，超时之后会再重试。</li>
</ol>
</li>
</ol>
<p><strong>CONFIRM_OPENED</strong></p>
<p>检查OPEN操作最终是否成功。</p>
<ol>
<li>如果Region成功OPEN
<ol>
<li>要看lastState是否就是CONFIRM_OPENED，是的话就完事了。</li>
<li>不是的话要再执行CLOSE操作。就像简图中的环上一样执行。有可能出现的一种情况是，当merge/split region的时候，要unassign一个region，但过程中RS crash了，就得先把这个region open了，再执行unassign操作，防止RS crash时数据丢失无法恢复。</li>
</ol>
</li>
<li>重试次数达到上限，直接结束</li>
<li>抹掉regionLocation，重新执行GET_ASSIGN_CANDIDATE</li>
</ol>
<p><strong>CLOSE</strong></p>
<ol>
<li>region当前的状态合理，通过CloseRegionProcedure，执行过程类似于OpenRegionProcedure。</li>
<li>不合理直接再去GET_ASSIGN_CANDIDATE走OPEN的逻辑</li>
</ol>
<p><strong>CONFIRM_CLOSED</strong></p>
<ol>
<li>如果Region当前状态是CLOSE，说明上一步执行成功了
<ol>
<li>如果lastState == CONFIRM_CLOSED，说明当前状态就是要求的最终状态，可以结束了。</li>
<li>否则，说明可能是move或者reopen操作，需要再assign region，去GET_ASSIGN_CANDIDATE走OPEN的逻辑。<strong>只有这一个地方回到GET_ASSIGN_CANDIDATE没有抹掉regionLocation</strong>。</li>
</ol>
</li>
<li>如果Region当前状态是CLOSING，说明close rpc执行失败了，rs没有回调通知（eg. rs重启了），一直等到了CloseRegionProcedure超时触发了TRSP继续执行，才走到了当前状态，需要再重新Close一次。</li>
<li>走到这里的Region可能是ABNORMALLY_CLOSED状态，应该只有RS crash才会导致这个状态。
<ol>
<li>如果非default region，ABNORMALLY_CLOSED可以被当作CLOSE处理，直接结束掉。只有开启了read region replicas功能才有这样的region。非default region不接收写操作，所以即使非正常close也不会造成数据丢失。</li>
<li>Region close异常，需要再open之后再正常close，保证数据不会丢失。原因和CONFIRM_OPENED里的逻辑类似，比如merge/split region的时候，要先close region，如果失败了，则必须先恢复region再重新close，避免数据丢失。</li>
</ol>
</li>
</ol>
<h3 id="问题-v2">问题</h3>
<p><strong>region merge/split是怎么处理的？</strong></p>
<p>region merge/split分别是由MergeTableRegionsProcedure和SplitTableRegionProcedure来执行的，它们步步骤都可以拆分成region assign/unassign，也就会拆成不同的TRSP去执行。</p>
<p>以 region split为例</p>
<ol>
<li>close父region（unassign）</li>
<li>处理好子region的信息保存之后</li>
<li>open子region（assign）</li>
</ol>
<h3 id="RegionRemoteProcedureBase">RegionRemoteProcedureBase</h3>
<p>RegionRemoteProcedureBase是OpenRegionProcedure和CloseRegionProcedure的父类，实现了基本的RPC相关功能。RegionRemoteProcedureBase与TRSP的配合主要也是利用了ProcedureEvent机制。</p>
<p><img src="RegionRemoteProcedureBase.png" alt="RegionRemoteProcedureBase时序图"></p>
<ol>
<li>addOperationToNode方法就是将RPC操作封装起来，放到一个集合中等待执行。</li>
<li>执行当前Region的ProcedureEvent.suspend操作，等待RPC执行完毕之后唤醒。</li>
<li>RSProcedureDispatcher会分批将RPC操作按RS和操作类型分类，批量请求某个RS。</li>
<li>RS执行完对应操作之后，通过reportTransition通知AM（当然中间还有Master），Region open/close完成。</li>
<li>然后AM就会唤醒RegionStateNode.event，继续执行RegionRemoteProcedureBase</li>
<li>RegionRemoteProcedureBase执行完成之后，继续执行TRSP后续步骤。</li>
</ol>
<h4 id="问题-v3">问题</h4>
<p><strong>RPC回调之后，怎么确定对应的哪个Procedure？</strong></p>
<p>RegionInfo -&gt; AM.regionStates (类型 RegionStates) -&gt; RegionStates.regionMap (类型 Map&lt;byte[], RegionStateNode&gt;) -&gt; RegionStateNode.procedure (类型 TRSP) -&gt; TRSP.remoteProc (类型 RegionRemoteProcedureBase) -&gt; RegionRemoteProcedureBase.reportTransition</p>
<p><strong>RS重启导致没有RPC回调，或者Master重启之后event队列丢失 怎么办？</strong></p>
<p>RegionRemoteProcedureBase有个超时限制，超时之后重新再执行一次。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>「TED」Listening to shame</title>
    <url>/blog/2020/03/23/TED-Listening-to-shame/</url>
    <content><![CDATA[<p><a href="https://www.ted.com/talks/brene_brown_listening_to_shame" target="_blank" rel="noopener">TED视频链接</a></p>
<a id="more"></a>

<p>I’m going to tell you a little bit about my TEDxHouston Talk. I woke up the morning after I gave that talk with the worst vulnerability hangover of my life. And I actually didn’t leave my house for about three days.</p>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/vulnerability" target="_blank" rel="noopener">vulnerability</a>  [ˌvʌlnərə’bɪləti]<br>n. 弱点;脆弱性;易伤性;可捕性</p>
</blockquote>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/hangover" target="_blank" rel="noopener">hangover</a>  [ˈhæŋoʊvər]<br>n. 宿醉;遗留的感觉;沿袭下来的风俗(或思想等)</p>
</blockquote>
<p>The first time I left was to meet a friend for lunch. And when I walked in, she was already at the table. I sat down, and she said, “God, you look like hell.” I said, “Thanks. I feel really – I’m not functioning.” And she said, “What’s going on?” And I said, “I just told 500 people that I became a researcher to avoid vulnerability. And that when being vulnerable emerged from my data, as absolutely essential to whole-hearted living, I told these 500 people that I had a breakdown. I had a slide that said ‘Breakdown.’ At what point did I think that was a good idea?”</p>
<blockquote>
<p>emerge from  [iˈmɜːrdʒ frəm]<br>（从…）露出，浮现;〈正〉来自，产生于，脱离<br>emerge from my data 我的数据表明</p>
</blockquote>
<blockquote>
<p>essential [ɪˈsenʃl]<br>adj. 完全必要的;必不可少的;极其重要的;本质的;基本的;根本的<br>n. 必不可少的东西;必需品;要点;要素;实质  </p>
</blockquote>
<blockquote>
<p>slideshow  幻灯片<br>make a slideshow, slide理解为一张幻灯片</p>
</blockquote>
<p>(Laughter)</p>
<p>And she said, “I saw your talk live-streamed. It was not really you. It was a little different than what you usually do. But it was great.” And I said, “This can’t happen. YouTube, they’re putting this thing on YouTube. And we’re going to be talking about 600, 700 people.”</p>
<blockquote>
<p>live-streamed 直播</p>
</blockquote>
<p>(Laughter)</p>
<p>And she said, “Well, I think it’s too late.”</p>
<p>And I said, “Let me ask you something.” And she said, “Yeah.” I said, “Do you remember when we were in college, really wild and kind of dumb?” She said, “Yeah.” I said, “Remember when we’d leave a really bad message on our ex-boyfriend’s answering machine? Then we’d have to break into his dorm room and then erase the tape?”</p>
<p>(Laughter)</p>
<p>And she goes, “Uh… no.”</p>
<p>(Laughter)</p>
<p>Of course, the only thing I could say at that point was, “Yeah, me neither. Yeah – me neither.”</p>
<p>And I’m thinking to myself, “Brené, what are you doing? Why did you bring this up? Have you lost your mind? Your sisters would be perfect for this.”</p>
<p>(Laughter)</p>
<p>So I looked back up and she said, “Are you really going to try to break in and steal the video before they put it on YouTube?”</p>
<p>(Laughter)</p>
<p>And I said, “I’m just thinking about it a little bit.”</p>
<p>(Laughter)</p>
<p>She said, “You’re like the worst vulnerability role model ever.”</p>
<p>(Laughter)</p>
<p>Then I looked at her and I said something that at the time felt a little dramatic, but ended up being more prophetic than dramatic. “If 500 turns into 1,000 or 2,000, my life is over.”</p>
<p>(Laughter)</p>
<p>I had no contingency plan for four million.</p>
<blockquote>
<p>contingency [kənˈtɪndʒənsi] plan 应急方案</p>
</blockquote>
<p>(Laughter)</p>
<p>And my life did end when that happened. And maybe the hardest part about my life ending is that I learned something hard about myself, and that was that, as much as I would be frustrated about not being able to get my work out to the world, there was a part of me that was working very hard to engineer staying small, staying right under the radar. But I want to talk about what I’ve learned.</p>
<p>There’s two things that I’ve learned in the last year. The first is: vulnerability is not weakness. And that myth is profoundly dangerous. Let me ask you honestly – and I’ll give you this warning, I’m trained as a therapist, so I can out-wait you uncomfortably – so if you could just raise your hand that would be awesome – how many of you honestly, when you’re thinking about doing or saying something vulnerable think, “God, vulnerability is weakness.” How many of you think of vulnerability and weakness synonymously? The majority of people. Now let me ask you this question: This past week at TED, how many of you, when you saw vulnerability up here, thought it was pure courage? Vulnerability is not weakness. I define vulnerability as emotional risk, exposure, uncertainty. It fuels our daily lives. And I’ve come to the belief – this is my 12th year doing this research – that vulnerability is our most accurate measurement of courage – to be vulnerable, to let ourselves be seen, to be honest.</p>
<p>One of the weird things that’s happened is, after the TED explosion, I got a lot of offers to speak all over the country – everyone from schools and parent meetings to Fortune 500 companies. And so many of the calls went like this, “Dr. Brown, we loved your TED talk. We’d like you to come in and speak. We’d appreciate it if you wouldn’t mention vulnerability or shame.”</p>
<p>(Laughter)</p>
<p>What would you like for me to talk about? There’s three big answers. This is mostly, to be honest with you, from the business sector: innovation, creativity and change.</p>
<p>(Laughter)</p>
<p>So let me go on the record and say, vulnerability is the birthplace of innovation, creativity and change.</p>
<p>(Applause)</p>
<p>To create is to make something that has never existed before. There’s nothing more vulnerable than that. Adaptability to change is all about vulnerability.</p>
<p>The second thing, in addition to really finally understanding the relationship between vulnerability and courage, the second thing I learned, is this: We have to talk about shame. And I’m going to be really honest with you. When I became a “vulnerability researcher” and that became the focus because of the TED talk – and I’m not kidding.</p>
<p>I’ll give you an example. About three months ago, I was in a sporting goods store buying goggles and shin guards and all the things that parents buy at the sporting goods store. About from a hundred feet away, this is what I hear: “Vulnerability TED! Vulnerability TED!”</p>
<p>(Laughter)</p>
<p>(Laughter ends)</p>
<p>I’m a fifth-generation Texan. Our family motto is “Lock and load.” I am not a natural vulnerability researcher. So I’m like, just keep walking, she’s on my six.</p>
<p>(Laughter)</p>
<p>And then I hear, “Vulnerability TED!” I turn around, I go, “Hi.” She’s right here and she said, “You’re the shame researcher who had the breakdown.”</p>
<p>(Laughter)</p>
<p>At this point, parents are, like, pulling their children close.</p>
<p>(Laughter)</p>
<p>“Look away.” And I’m so worn out at this point in my life, I look at her and I actually say, “It was a fricking spiritual awakening.”</p>
<p>(Laughter)</p>
<p>(Applause)</p>
<p>And she looks back and does this, “I know.”</p>
<p>(Laughter)</p>
<p>And she said, “We watched your TED talk in my book club. Then we read your book and we renamed ourselves ‘The Breakdown Babes.’”</p>
<p>(Laughter)</p>
<p>And she said, “Our tagline is: ‘We’re falling apart and it feels fantastic.’”</p>
<p>(Laughter)</p>
<p>You can only imagine what it’s like for me in a faculty meeting.</p>
<p>(Sighs)</p>
<p>So when I became Vulnerability TED, like an action figure – Like Ninja Barbie, but I’m Vulnerability TED – I thought, I’m going to leave that shame stuff behind, because I spent six years studying shame before I started writing and talking about vulnerability. And I thought, thank God, because shame is this horrible topic, no one wants to talk about it. It’s the best way to shut people down on an airplane. “What do you do?” “I study shame.” “Oh.”</p>
<p>(Laughter)</p>
<p>And I see you.</p>
<p>(Laughter)</p>
<p>But in surviving this last year, I was reminded of a cardinal rule – not a research rule, but a moral imperative from my upbringing – “you’ve got to dance with the one who brung ya”. And I did not learn about vulnerability and courage and creativity and innovation from studying vulnerability. I learned about these things from studying shame. And so I want to walk you in to shame. Jungian analysts call shame the swampland of the soul. And we’re going to walk in. And the purpose is not to walk in and construct a home and live there. It is to put on some galoshes – and walk through and find our way around. Here’s why.</p>
<p>We heard the most compelling call ever to have a conversation in this country, and I think globally, around race, right? Yes? We heard that. Yes? Cannot have that conversation without shame. Because you cannot talk about race without talking about privilege. And when people start talking about privilege, they get paralyzed by shame. We heard a brilliant simple solution to not killing people in surgery, which is, have a checklist. You can’t fix that problem without addressing shame, because when they teach those folks how to suture, they also teach them how to stitch their self-worth to being all-powerful. And all-powerful folks don’t need checklists.</p>
<p>And I had to write down the name of this TED Fellow so I didn’t mess it up here. Myshkin Ingawale, I hope I did right by you.</p>
<p>(Applause)</p>
<p>I saw the TED Fellows my first day here. And he got up and he explained how he was driven to create some technology to help test for anemia, because people were dying unnecessarily. And he said, “I saw this need. So you know what I did? I made it.” And everybody just burst into applause, and they were like “Yes!” And he said, “And it didn’t work.</p>
<p>(Laughter)</p>
<p>And then I made it 32 more times, and then it worked.”</p>
<p>You know what the big secret about TED is? I can’t wait to tell people this. I guess I’m doing it right now.</p>
<p>(Laughter)</p>
<p>This is like the failure conference.</p>
<p>(Laughter)</p>
<p>No, it is.</p>
<p>(Applause)</p>
<p>You know why this place is amazing? Because very few people here are afraid to fail. And no one who gets on the stage, so far that I’ve seen, has not failed. I’ve failed miserably, many times. I don’t think the world understands that, because of shame.</p>
<p>There’s a great quote that saved me this past year by Theodore Roosevelt. A lot of people refer to it as the “Man in the Arena” quote. And it goes like this: “It is not the critic who counts. It is not the man who sits and points out how the doer of deeds could have done things better and how he falls and stumbles. The credit goes to the man in the arena whose face is marred with dust and blood and sweat. But when he’s in the arena, at best, he wins, and at worst, he loses, but when he fails, when he loses, he does so daring greatly.”</p>
<p>And that’s what this conference, to me, is about. Life is about daring greatly, about being in the arena. When you walk up to that arena and you put your hand on the door, and you think, “I’m going in and I’m going to try this,” shame is the gremlin who says, “Uh, uh. You’re not good enough. You never finished that MBA. Your wife left you. I know your dad really wasn’t in Luxembourg, he was in Sing Sing. I know those things that happened to you growing up. I know you don’t think that you’re pretty, smart, talented or powerful enough. I know your dad never paid attention, even when you made CFO.” Shame is that thing.</p>
<p>And if we can quiet it down and walk in and say, “I’m going to do this,” we look up and the critic that we see pointing and laughing, 99 percent of the time is who? Us. Shame drives two big tapes – “never good enough” – and, if you can talk it out of that one, “who do you think you are?” The thing to understand about shame is, it’s not guilt. Shame is a focus on self, guilt is a focus on behavior. Shame is “I am bad.” Guilt is “I did something bad.” How many of you, if you did something that was hurtful to me, would be willing to say, “I’m sorry. I made a mistake?” How many of you would be willing to say that? Guilt: I’m sorry. I made a mistake. Shame: I’m sorry. I am a mistake.</p>
<p>There’s a huge difference between shame and guilt. And here’s what you need to know. Shame is highly, highly correlated with addiction, depression, violence, aggression, bullying, suicide, eating disorders. And here’s what you even need to know more. Guilt, inversely correlated with those things. The ability to hold something we’ve done or failed to do up against who we want to be is incredibly adaptive. It’s uncomfortable, but it’s adaptive.</p>
<p>The other thing you need to know about shame is it’s absolutely organized by gender. If shame washes over me and washes over Chris, it’s going to feel the same. Everyone sitting in here knows the warm wash of shame. We’re pretty sure that the only people who don’t experience shame are people who have no capacity for connection or empathy. Which means, yes, I have a little shame; no, I’m a sociopath. So I would opt for, yes, you have a little shame. Shame feels the same for men and women, but it’s organized by gender.</p>
<p>For women, the best example I can give you is Enjoli, the commercial. “I can put the wash on the line, pack the lunches, hand out the kisses and be at work at five to nine. I can bring home the bacon, fry it up in the pan and never let you forget you’re a man.” For women, shame is, do it all, do it perfectly and never let them see you sweat. I don’t know how much perfume that commercial sold, but I guarantee you, it moved a lot of antidepressants and anti-anxiety meds.</p>
<p>(Laughter)</p>
<p>Shame, for women, is this web of unobtainable, conflicting, competing expectations about who we’re supposed to be. And it’s a straight-jacket.</p>
<p>For men, shame is not a bunch of competing, conflicting expectations. Shame is one, do not be perceived as what? Weak. I did not interview men for the first four years of my study. It wasn’t until a man looked at me after a book signing, and said, “I love what say about shame, I’m curious why you didn’t mention men.” And I said, “I don’t study men.” And he said, “That’s convenient.”</p>
<p>(Laughter)</p>
<p>And I said, “Why?” And he said, “Because you say to reach out, tell our story, be vulnerable. But you see those books you just signed for my wife and my three daughters?” I said, “Yeah.” “They’d rather me die on top of my white horse than watch me fall down. When we reach out and be vulnerable, we get the shit beat out of us. And don’t tell me it’s from the guys and the coaches and the dads. Because the women in my life are harder on me than anyone else.”</p>
<p>So I started interviewing men and asking questions. And what I learned is this: You show me a woman who can actually sit with a man in real vulnerability and fear, I’ll show you a woman who’s done incredible work. You show me a man who can sit with a woman who’s just had it, she can’t do it all anymore, and his first response is not, “I unloaded the dishwasher!”</p>
<p>(Laughter)</p>
<p>But he really listens – because that’s all we need – I’ll show you a guy who’s done a lot of work.</p>
<p>Shame is an epidemic in our culture. And to get out from underneath it – to find our way back to each other, we have to understand how it affects us and how it affects the way we’re parenting, the way we’re working, the way we’re looking at each other. Very quickly, some research by Mahalik at Boston College. He asked, what do women need to do to conform to female norms? The top answers in this country: nice, thin, modest and use all available resources for appearance.</p>
<p>(Laughter)</p>
<p>When he asked about men, what do men in this country need to do to conform with male norms, the answers were: always show emotional control, work is first, pursue status and violence.</p>
<p>If we’re going to find our way back to each other, we have to understand and know empathy, because empathy’s the antidote to shame. If you put shame in a Petri dish, it needs three things to grow exponentially: secrecy, silence and judgment. If you put the same amount in a Petri dish and douse it with empathy, it can’t survive. The two most powerful words when we’re in struggle: me too.</p>
<p>And so I’ll leave you with this thought. If we’re going to find our way back to each other, vulnerability is going to be that path. And I know it’s seductive to stand outside the arena, because I think I did it my whole life, and think to myself, I’m going to go in there and kick some ass when I’m bulletproof and when I’m perfect. And that is seductive. But the truth is, that never happens. And even if you got as perfect as you could and as bulletproof as you could possibly muster when you got in there, that’s not what we want to see. We want you to go in. We want to be with you and across from you. And we just want, for ourselves and the people we care about and the people we work with, to dare greatly.</p>
<p>So thank you all very much. I really appreciate it.</p>
<p>(Applause)</p>
]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>TED</tag>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase不停服跨集群数据迁移</title>
    <url>/blog/2020/03/14/HBase%E4%B8%8D%E5%81%9C%E6%9C%8D%E8%B7%A8%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<p>最近接到两个用户提的JIRA，都是目前数据只存在了一个srv集群，需要将数据备份prc集群做数据备份或者离线计算，同时两集群之间还要做数据实时同步。所以在这里记录下操作过程并介绍下原因。</p>
<a id="more"></a>

<p>集群之间增量数据同步自然是利用replication，单向同步配置一个peer，双向同步配置两个peer就好。</p>
<p>存量数据使用snapshot同步。</p>
<h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><p><strong>0. 准备工作</strong></p>
<p>在prc集群新建namespace</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create_namespace &#39;ns&#39;</span><br></pre></td></tr></table></figure>

<p><strong>1. 新建srv到prc集群的peer</strong></p>
<p><strong>注意peer要disable掉</strong></p>
<p>如果之前没有srv到prc集群的peer，直接新建peer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 复制namespace</span><br><span class="line">add_peer &#39;1&#39;, CLUSTER_KEY &#x3D;&gt; &quot;zk1,zk2,zk3:11000:&#x2F;hbase&#x2F;prc&quot;, STATE &#x3D;&gt; &quot;DISABLED&quot;, NAMESPACES &#x3D;&gt; [&quot;ns&quot;]</span><br><span class="line">&#x2F;&#x2F; 复制table</span><br><span class="line">add_peer &#39;1&#39;, CLUSTER_KEY &#x3D;&gt; &quot;zk1,zk2,zk3:11000:&#x2F;hbase&#x2F;prc&quot;, STATE &#x3D;&gt; &quot;DISABLED&quot;, TABLE_CFS &#x3D;&gt; &#123; &quot;ns:table1&quot; &#x3D;&gt; [] &#125;</span><br></pre></td></tr></table></figure>
<p>如果之前已经有srv到prc集群的peer，在peer中加入要操作的namespace</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">disable_peer &#39;1&#39;</span><br><span class="line">&#x2F;&#x2F; 新加namespace</span><br><span class="line">append_peer_namespaces &#39;1&#39;, [&quot;ns&quot;]</span><br><span class="line">&#x2F;&#x2F; 新加table</span><br><span class="line">append_peer_tableCFs &#39;1&#39;, &#123; &quot;ns:table1&quot; &#x3D;&gt; []&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2. 打snapshot</strong></p>
<p>将要同步的表打snapshot，如果复制整个namespace，要一个一个打</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">snapshot &#39;ns:table1&#39;, &#39;ns_table1_snapshot&#39;</span><br></pre></td></tr></table></figure>

<p><strong>3. 将snapshot复制到prc集群</strong></p>
<p>使用<code>ExportSnapshot</code>将snapshot从srv复制到prc集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;hbase --config &#x2F;path&#x2F;to&#x2F;conf org.apache.hadoop.hbase.snapshot.ExportSnapshot -copy-from hdfs:&#x2F;&#x2F;srv&#x2F;hbase&#x2F;srv -copy-to hdfs:&#x2F;&#x2F;prc&#x2F;hbase&#x2F;prc -snapshot ns_table1_snapshot -mappers 100 -bandwidth 512 -overwrite &gt;&gt; copy.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p><strong>4. 在prc集群使用snapshot生成表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clone_snapshot &#39;ns_table1_snapshot&#39;, &#39;ns:table1&#39;</span><br></pre></td></tr></table></figure>

<p><strong>5. 开启peer</strong></p>
<p>将srv到prc集群的peer打开</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">enable_peer &#39;1&#39;</span><br></pre></td></tr></table></figure>

<p><strong>6. 其他</strong></p>
<p>在peer集群加权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grant &#39;kerberos_name&#39;, &#39;RW&#39;, &#39;@ns1&#39;</span><br></pre></td></tr></table></figure>

<p>如果需要俩集群双向同步，在增加prc到srv集群的peer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add_peer &#39;2&#39;, CLUSTER_KEY &#x3D;&gt; &quot;zk1,zk2,zk3:11000:&#x2F;hbase&#x2F;srv&quot;, STATE &#x3D;&gt; &quot;ENABLED&quot;, NAMESPACES &#x3D;&gt; [&quot;ns&quot;]</span><br></pre></td></tr></table></figure>

<h2 id="操作原因"><a href="#操作原因" class="headerlink" title="操作原因"></a>操作原因</h2><p>数据复制自然不能丢数据，也就是要保证snapshot的存量数据和replication的增量数据之间不能有间隙，但其实可以有重叠。</p>
<p>我们先新建replication peer，将现有的WAL和之后再生成的WAL都加进该peer的复制队列中，disable掉peer的原因则是prc集群还没有新建表，replication开始复制则会出现<code>Table xxx does not exist</code>的报错。这一步如果是修改的原有peer，则会导致replication延迟增加。</p>
<p>之后再打snapshot便不用担心数据有丢失了，此时replication的数据与snapshot的数据已经有重叠了。</p>
<p>最后总结一句，一定要先新建replication peer，然后在打snapshot。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 学习笔记 Day3：MySQL事务隔离</title>
    <url>/blog/2020/01/26/MySQL-notes-Day3/</url>
    <content><![CDATA[<p>极客时间MySQL实战45讲 <a href="https://time.geekbang.org/column/article/68963" target="_blank" rel="noopener">03 | 事务隔离：为什么你改了我还看不见？</a> 学习笔记</p>
<a id="more"></a>

<h3 id="事务的隔离级别"><a href="#事务的隔离级别" class="headerlink" title="事务的隔离级别"></a>事务的隔离级别</h3><table>
<thead>
<tr>
<th>隔离级别</th>
<th>描述</th>
<th>主要问题</th>
</tr>
</thead>
<tbody><tr>
<td>读未提交（Read Uncommitted）</td>
<td>一个事务还没提交时，它做的变更就能被别的事务看到</td>
<td>脏读</td>
</tr>
<tr>
<td>读提交（Read Committed）</td>
<td>一个事务提交之后，它做的变更才会被其他事务看到</td>
<td>不可重复读</td>
</tr>
<tr>
<td>可重复读（Repeatable Read）</td>
<td>一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的</td>
<td>幻读</td>
</tr>
<tr>
<td>串行化（Serializable）</td>
<td>同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行</td>
<td></td>
</tr>
</tbody></table>
<h3 id="事务隔离的实现"><a href="#事务隔离的实现" class="headerlink" title="事务隔离的实现"></a>事务隔离的实现</h3><p><img src="day3-tx-readview.png" alt=""></p>
<p>在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作，通过回滚操作，都可以得到前一个状态的值。在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view，要得到 read-view A 的值，就要依次执行图中所有的回滚操作。</p>
<p>记录这些回滚操作的回滚日志，只有当系统里没有比这个回滚日志更早的 read-view 的时候才会删除。</p>
<p>所以长事务除了会影响访问速度之外，还会造成大量回滚日志无法删除，占用空间。</p>
<h3 id="业务开发中对长事务的思考"><a href="#业务开发中对长事务的思考" class="headerlink" title="业务开发中对长事务的思考"></a>业务开发中对长事务的思考</h3><p>手动开闭事务，减少在开启过程中进行的无必要操作，自然是可以缩短事务时间。但对于业务开发来说，这无疑增加了开发成本。本来业务项目中使用各种ORM框架就是为了以面向对象的思维方式来操作关系型数据库，减少在业务代码中出现过多的数据库细节。而手动开闭事务，既对业务代码造成侵入，也无法保证逻辑出错时事务能正常关闭。</p>
<p>所以事务的开闭还是应该由框架控制，除了Spring那样通过在方法上加注解的形式，之前在百度的工作中还见过一种办法，大概类似于下面这样。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> T &lt;T&gt; executeTx(Callable&lt;T&gt; callable) &#123;</span><br><span class="line">    T r;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 开启事务</span></span><br><span class="line">        r = callable.call();</span><br><span class="line">        <span class="comment">// 提交事务</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="comment">// 回滚事务</span></span><br><span class="line">        <span class="keyword">throw</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在需要事务的时候通过执行<code>executeTx</code>开启，而不是单纯的将service层的方法作为事务执行的单位。比如可以在事务执行前，将需要用的的数据都查询出来，进行相应的处理计算，数据最终要入库时再调用<code>executeTx</code>使用事务。</p>
]]></content>
      <categories>
        <category>MySQL学习笔记</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven项目以Shaded形式引入第三方依赖库</title>
    <url>/blog/2020/01/13/Shaded-Thirdparty-Dependencies/</url>
    <content><![CDATA[<p>最近有用户反馈，他的项目中同时使用了HBase和一个RPC框架，HBase依赖2.5.0的protobuf，RPC框架依赖3.7的protobuf，导致他的项目编译都失败。0.98版本的HBase还是使用的原生的protobuf-java依赖，2.0版本才使用了shaded形式的protobuf，所以我们决定自己提供以shaded形式引入protobuf的HBase Client。</p>
<p>相信这个问题不只是出现在HBase中，或者出现在与protobuf相关的项目中，其实当我们项目间接依赖了像protobuf、netty等大版本之间互不兼容的框架，甚至guava这种某些接口不兼容的框架，都有可能出现类似的问题。这里也是提供一个可以参考的解决方法。</p>
<p>方法上，参考了HBase 2.0之后对第三方依赖的处理，简单来说就是将常用第三方依赖的代码负责一份，修改所有类的package，发布一个自己的artifact到仓库中。这样肯定就不会再依赖冲突了。</p>
<p>而protobuf的依赖处理起来则比较麻烦一点，除了修改原生protobuf类的package之外，还需要处理proto生成的Java文件。所以本文就以protobuf为例，提供第三方依赖冲突的解决方案。因为都是使用Maven插件实现，所以也只对Maven项目有用，相信其他项目也有类似的解决办法。</p>
<p>本文相关代码都已放在<a href="https://github.com/ddupg/demos/tree/master/shaded" target="_blank" rel="noopener">Github</a></p>
<a id="more"></a>

<h2 id="生成shaded依赖包"><a href="#生成shaded依赖包" class="headerlink" title="生成shaded依赖包"></a>生成shaded依赖包</h2><p>可以直接参考<a href="https://github.com/ddupg/demos/blob/master/shaded/pom.xml" target="_blank" rel="noopener">pom.xml的代码</a>，其中主要使用到的插件是以下几个：</p>
<ul>
<li>maven-dependency-plugin 将protobuf依赖包下载下来并解包放在src/main/java目录下，变成自己项目的源码</li>
<li>maven-shade-plugin 在打包阶段，修改protobuf类的package名。例如HBase项目是将<code>com.google.protobuf</code>改为<code>org.apache.hbase.thirdparty.com.google.protobuf</code>，我的代码中是将<code>com.google.protobuf</code>改为<code>ddupg.demo.thirdparty.com.google.protobuf</code></li>
<li>maven-clean-plugin 在操作之前，清理下之前生成的文件</li>
</ul>
<p>修改好pom文件之后，直接mvn clean deploy发布到仓库就好了，中央仓库和私有仓库都是一样的。</p>
<p>可以解压target目录下打好的包，就可以看到class文件的package路径已经变化了。因为是在package阶段才修改的package名，所以直接看target/generated-sources下的class文件其实是不变的。</p>
<p>普通的第三方依赖项目使用这一步其实就可以解决了。</p>
<h2 id="生成proto文件"><a href="#生成proto文件" class="headerlink" title="生成proto文件"></a>生成proto文件</h2><p>解决掉了protobuf的源文件，下一步就是处理proto生成的文件了。</p>
<p>因为需要依赖上一步生成的shaded包，而我又没有将它deploy到中央仓库，也没有自己的私人仓库，所以便使用了<a href="https://github.com/apache/hbase-thirdparty" target="_blank" rel="noopener">HBase项目的第三方依赖</a>，道理都一样，替换一下dependency就好了。</p>
<p>可以直接参考<a href="https://github.com/ddupg/demos/blob/master/shaded/shaded-protocol/pom.xml" target="_blank" rel="noopener">pom.xml的代码</a>.</p>
<p>这一步主要使用的插件：</p>
<ul>
<li>protobuf-maven-plugin 生成proto文件，这一步生成的文件依旧使用的是原生protobuf。</li>
<li>com.google.code.maven-replacer-plugin:replacer 替换源代码，将protobuf原生的package路径<code>com.google.protobuf</code>改成第一步中修改后的package名。因为我在这里使用的HBase的第三方依赖项目，所以就是将<code>com.google.protobuf</code>改为<code>org.apache.hbase.thirdparty.com.google.protobuf</code></li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.xolstice.org/protobuf-maven-plugin/" target="_blank" rel="noopener">Maven Protocol Buffers Plugin</a></li>
<li><a href="https://maven.apache.org/plugins/maven-shade-plugin/" target="_blank" rel="noopener">Apache Maven Shade Plugin</a></li>
<li><a href="https://code.google.com/archive/p/maven-replacer-plugin/" target="_blank" rel="noopener">maven-replacer-plugin</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>「TED」How you can help transform the internet into a place of trust</title>
    <url>/blog/2020/01/03/ted-how-you-can-help-transform-the-internet-into-a-place-of-trust/</url>
    <content><![CDATA[<p><a href="https://www.ted.com/talks/claire_wardle_how_you_can_help_transform_the_internet_into_a_place_of_trust/" target="_blank" rel="noopener">TED视频链接</a></p>
<a id="more"></a>

<p>No matter who you are or where you live, I’m guessing that you have at least one relative that likes to forward those emails. You know the ones I’m talking about – the ones with dubious claims or conspiracy videos. And you’ve probably already muted them on Facebook for sharing social posts like this one. </p>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/relative" target="_blank" rel="noopener">relative</a>  [ˈrelətɪv]<br>adj. 相比较而言的;比较的;相对的;相关联的;相比之下存在(或有)的<br>n. 亲戚;亲属;同类事物</p>
</blockquote>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/dubious" target="_blank" rel="noopener">dubious</a> [ˈduːbiəs]<br>adj. 怀疑;无把握;拿不准;可疑的;不可信的;靠不住的;不诚实的;不确定的;不一定好的</p>
</blockquote>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/conspiracy" target="_blank" rel="noopener">conspiracy</a> [kənˈspɪrəsi]<br>n. 密谋策划;阴谋</p>
</blockquote>
<blockquote>
<p><a href="https://fanyi.baidu.com/#en/zh/mute" target="_blank" rel="noopener">mute</a> [mjuːt]<br>adj. 沉默的;不出声的;无声的;哑的<br>n. 弱音器;哑巴<br>v. 消音;减音;减弱(尤指乐器)的声音;减弱;缓解<br>这里是<strong>屏蔽</strong>的意思</p>
</blockquote>
<p>It’s an image of a banana with a strange red cross running through the center. And the text around it is warning people not to eat fruits that look like this, suggesting they’ve been injected with blood contaminated with the HIV virus. And the social share message above it simply says, “Please forward to save lives.” Now, fact-checkers have been debunking this one for years, but it’s one of those rumors that just won’t die. A zombie rumor. And, of course, it’s entirely false. </p>
<p>It might be tempting to laugh at an example like this, to say, “Well, who would believe this, anyway?” But the reason it’s a zombie rumor is because it taps into people’s deepest fears about their own safety and that of the people they love. And if you spend as enough time as I have looking at misinformation, you know that this is just one example of many that taps into people’s deepest fears and vulnerabilities. </p>
<p>Every day, across the world, we see scores of new memes on Instagram encouraging parents not to vaccinate their children. We see new videos on YouTube explaining that climate change is a hoax. And across all platforms, we see endless posts designed to demonize others on the basis of their race, religion or sexuality. </p>
<p>Welcome to one of the central challenges of our time. How can we maintain an internet with freedom of expression at the core, while also ensuring that the content that’s being disseminated doesn’t cause irreparable harms to our democracies, our communities and to our physical and mental well-being? Because we live in the information age, yet the central currency upon which we all depend – information – is no longer deemed entirely trustworthy and, at times, can appear downright dangerous. This is thanks in part to the runaway growth of social sharing platforms that allow us to scroll through, where lies and facts sit side by side, but with none of the traditional signals of trustworthiness. </p>
<p>And goodness – our language around this is horribly muddled. People are still obsessed with the phrase “fake news,” despite the fact that it’s extraordinarily unhelpful and used to describe a number of things that are actually very different: lies, rumors, hoaxes, conspiracies, propaganda. And I really wish we could stop using a phrase that’s been co-opted by politicians right around the world, from the left and the right, used as a weapon to attack a free and independent press. </p>
<p>(Applause) </p>
<p>Because we need our professional news media now more than ever. And besides, most of this content doesn’t even masquerade as news. It’s memes, videos, social posts. And most of it is not fake; it’s misleading. We tend to fixate on what’s true or false. But the biggest concern is actually the weaponization of context. Because the most effective disinformation has always been that which has a kernel of truth to it. </p>
<p>Let’s take this example from London, from March 2017, a tweet that circulated widely in the aftermath of a terrorist incident on Westminster Bridge. This is a genuine image, not fake. The woman who appears in the photograph was interviewed afterwards, and she explained that she was utterly traumatized. She was on the phone to a loved one, and she wasn’t looking at the victim out of respect. But it still was circulated widely with this Islamophobic framing, with multiple hashtags, including: #BanIslam. Now, if you worked at Twitter, what would you do? Would you take that down, or would you leave it up? My gut reaction, my emotional reaction, is to take this down. I hate the framing of this image. But freedom of expression is a human right, and if we start taking down speech that makes us feel uncomfortable, we’re in trouble. </p>
<p>And this might look like a clear-cut case, but, actually, most speech isn’t. These lines are incredibly difficult to draw. What’s a well-meaning decision by one person is outright censorship to the next. What we now know is that this account, Texas Lone Star, was part of a wider Russian disinformation campaign, one that has since been taken down. Would that change your view? It would mine, because now it’s a case of a coordinated campaign to sow discord. And for those of you who’d like to think that artificial intelligence will solve all of our problems, I think we can agree that we’re a long way away from AI that’s able to make sense of posts like this. </p>
<p>So I’d like to explain three interlocking issues that make this so complex and then think about some ways we can consider these challenges. First, we just don’t have a rational relationship to information, we have an emotional one. It’s just not true that more facts will make everything OK, because the algorithms that determine what content we see, well, they’re designed to reward our emotional responses. And when we’re fearful, oversimplified narratives, conspiratorial explanations and language that demonizes others is far more effective. And besides, many of these companies, their business model is attached to attention, which means these algorithms will always be skewed towards emotion. </p>
<p>Second, most of the speech I’m talking about here is legal. It would be a different matter if I was talking about child sexual abuse imagery or content that incites violence. It can be perfectly legal to post an outright lie. But people keep talking about taking down “problematic” or “harmful” content, but with no clear definition of what they mean by that, including Mark Zuckerberg, who recently called for global regulation to moderate speech. And my concern is that we’re seeing governments right around the world rolling out hasty policy decisions that might actually trigger much more serious consequences when it comes to our speech. And even if we could decide which speech to take up or take down, we’ve never had so much speech. Every second, millions of pieces of content are uploaded by people right around the world in different languages, drawing on thousands of different cultural contexts. We’ve simply never had effective mechanisms to moderate speech at this scale, whether powered by humans or by technology. </p>
<p>And third, these companies – Google, Twitter, Facebook, WhatsApp – they’re part of a wider information ecosystem. We like to lay all the blame at their feet, but the truth is, the mass media and elected officials can also play an equal role in amplifying rumors and conspiracies when they want to. As can we, when we mindlessly forward divisive or misleading content without trying. We’re adding to the pollution. </p>
<p>I know we’re all looking for an easy fix. But there just isn’t one. Any solution will have to be rolled out at a massive scale, internet scale, and yes, the platforms, they’re used to operating at that level. But can and should we allow them to fix these problems? They’re certainly trying. But most of us would agree that, actually, we don’t want global corporations to be the guardians of truth and fairness online. And I also think the platforms would agree with that. And at the moment, they’re marking their own homework. They like to tell us that the interventions they’re rolling out are working, but because they write their own transparency reports, there’s no way for us to independently verify what’s actually happening. </p>
<p>(Applause) </p>
<p>And let’s also be clear that most of the changes we see only happen after journalists undertake an investigation and find evidence of bias or content that breaks their community guidelines. So yes, these companies have to play a really important role in this process, but they can’t control it. </p>
<p>So what about governments? Many people believe that global regulation is our last hope in terms of cleaning up our information ecosystem. But what I see are lawmakers who are struggling to keep up to date with the rapid changes in technology. And worse, they’re working in the dark, because they don’t have access to data to understand what’s happening on these platforms. And anyway, which governments would we trust to do this? We need a global response, not a national one. </p>
<p>So the missing link is us. It’s those people who use these technologies every day. Can we design a new infrastructure to support quality information? Well, I believe we can, and I’ve got a few ideas about what we might be able to actually do. So firstly, if we’re serious about bringing the public into this, can we take some inspiration from Wikipedia? They’ve shown us what’s possible. Yes, it’s not perfect, but they’ve demonstrated that with the right structures, with a global outlook and lots and lots of transparency, you can build something that will earn the trust of most people. Because we have to find a way to tap into the collective wisdom and experience of all users. This is particularly the case for women, people of color and underrepresented groups. Because guess what? They are experts when it comes to hate and disinformation, because they have been the targets of these campaigns for so long. And over the years, they’ve been raising flags, and they haven’t been listened to. This has got to change. So could we build a Wikipedia for trust? Could we find a way that users can actually provide insights? They could offer insights around difficult content-moderation decisions. They could provide feedback when platforms decide they want to roll out new changes. </p>
<p>Second, people’s experiences with the information is personalized. My Facebook news feed is very different to yours. Your YouTube recommendations are very different to mine. That makes it impossible for us to actually examine what information people are seeing. So could we imagine developing some kind of centralized open repository for anonymized data, with privacy and ethical concerns built in? Because imagine what we would learn if we built out a global network of concerned citizens who wanted to donate their social data to science. Because we actually know very little about the long-term consequences of hate and disinformation on people’s attitudes and behaviors. And what we do know, most of that has been carried out in the US, despite the fact that this is a global problem. We need to work on that, too. </p>
<p>And third, can we find a way to connect the dots? No one sector, let alone nonprofit, start-up or government, is going to solve this. But there are very smart people right around the world working on these challenges, from newsrooms, civil society, academia, activist groups. And you can see some of them here. Some are building out indicators of content credibility. Others are fact-checking, so that false claims, videos and images can be down-ranked by the platforms. </p>
<p>A nonprofit I helped to found, First Draft, is working with normally competitive newsrooms around the world to help them build out investigative, collaborative programs. And Danny Hillis, a software architect, is designing a new system called The Underlay, which will be a record of all public statements of fact connected to their sources, so that people and algorithms can better judge what is credible. And educators around the world are testing different techniques for finding ways to make people critical of the content they consume. All of these efforts are wonderful, but they’re working in silos, and many of them are woefully underfunded. </p>
<p>There are also hundreds of very smart people working inside these companies, but again, these efforts can feel disjointed, because they’re actually developing different solutions to the same problems. </p>
<p>How can we find a way to bring people together in one physical location for days or weeks at a time, so they can actually tackle these problems together but from their different perspectives? So can we do this? Can we build out a coordinated, ambitious response, one that matches the scale and the complexity of the problem? I really think we can. Together, let’s rebuild our information commons. </p>
<p>Thank you. </p>
<p>(Applause) </p>
]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>TED</tag>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title>初步了解HBase Region Replicas</title>
    <url>/blog/2019/12/05/Initial-Analysis-Region-Replicas/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>CAP原理中，指出对于一个分布式系统来说，不可能同时满足一致性 (Consistency)、可用性（Availability）、分区容错性（Partition tolerance），而HBase则被设计成一个CP系统，保证了强一致性的同时，选择牺牲了一定的可用性。</p>
<p>在对HBase的压测中，很容易发现虽然HBase的平均读写延迟很低，但却存在很高的毛刺，P99、P999延迟很高，主要的一个影响因素则是单点的GC，另外Region的MTTR（平均修复时间）也较高，一旦某个RegionServer宕机或某个Region出现问题，甚至是一次Full GC，都有可能出现较长时间的不可用，影响可用性。</p>
<p>HBase的Read Region Replicas功能，提供一个或多个副本，在region恢复期间或请求时间过长时，支持最终一致性的读服务。在一些不要求强一致性的应用中，可以通过此功能来提高可用性降低读请求延迟。</p>
<p>为了实现高可用读，HBase提供了一个feature，叫<code>region replication</code>。在这种模型下，表的每个region，都会有多个副本，分布在不同的RegionServer上。默认region replication为1，此时与之前的region模型并无不同。当region replication被设置为2或更多时，Master将会assign所有region的secondary region，Load Balancer会保证同一个region的多个备份会被分散在不同的RegionServer上。</p>
<p>一个region的所有副本都有一个唯一的replica_id。replica_id=0的是primary region（和之前模型中唯一的region一样），其他的副本region被都叫做secondary region。</p>
<p>primary region，支持读写请求；secondary region，只支持读请求。如此设计保证primary region依旧具有强一致性，同时提高读可用性。但也因为写请求只有primary region可以处理，所以写请求依然会因为primary region不可用而被阻塞，HBase的写可用性依然没有得到改善。</p>
<a id="more"></a>

<h2 id="Timeline-Consistency"><a href="#Timeline-Consistency" class="headerlink" title="Timeline Consistency"></a>Timeline Consistency</h2><p>在该功能的实现中，HBase提供了一种支持<strong>单次读请求</strong>的一致性定义。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> Consistency &#123;</span><br><span class="line">    STRONG,</span><br><span class="line">    TIMELINE</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HBase默认的就是<code>Consistency.STRONG</code>强一致性模型，与之前的模型一样，所有读写操作都通primary region完成。</p>
<p>而当client使用<code>Consistency.TIMELINE</code>的一致性发起读请求时，会首先向primary region发起请求，一定时间内没有返回响应，则同时并发向所有的secondary region发起请求，最终采用率先返回的请求。为了区分最终的响应是否来自secondary region，在<code>Result</code>中增加了<code>stale</code>的boolean属性，<code>true</code>则表示来自secondary region。</p>
<p>从语义上讲，HBase的TIMELINE一致性并不同于常见的最终一致性解决方案。</p>
<ul>
<li>即使存在多副本，也不需要考虑副本之间数据冲突的问题。</li>
<li>secondary region接收primary region同步的数据，按同样顺序处理数据，所以secondary region总是primary region在之前某个时刻的快照。从这一点上看，更像是RDBMS（关系型数据库管理系统）的复制、或是HBase多数据中心多集群之间的复制。</li>
<li>另一方面，client可以自行决定是否需要读取最新数据，自行决定使用哪一种一致性来满足功能需求。</li>
<li>client依然会读到乱序的数据，比如多次请求发往了不同的region。目前并没有类似于事务的东西来解决这个问题。</li>
</ul>
<p><img src="timeline_consistency.png" alt="Timeline Consistency"></p>
<p>根据上图我们来更好的理解TIMELINE的语义。首先client1按顺序写了x=1,x=2,x=3，primary region也按写入顺序处理，并将WAL同步给其他secondary region（一种数据同步方式，后面会再讲）。在图中注意到，replica_1只接收到两次更新，所以最终数据是x=2，replica_2只接收到1次更新，数据是x=1。</p>
<p>如果client1使用STRONG一致性来读数据，都只会和primary region交互，数据都是最新值x=3。可如果使用TIMELINE一致性读取数据，有可能和所有副本做交互，最终获得的数据1、2、3都有可能。如果client请求多次，甚至可能出现数据回退，即第1次请求获得x=2，第2次请求则获得了x=1。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p><img src="meta.png" alt="一个region replication为2的表在meta表中的列" title="一个region replication为2的表在meta表中的列"></p>
<p>在上图中，是一个region replication为2的表在meta表中info列族下的列，可以看到有一些名为info:xxx_0001的列，这些列存储的数据就是replica_id=1的secondary region的数据。同理，当region的备份数量更多时，meta表中名为info:xxx_0002、info:xxx_0003的列存储的则为replica_id为2、3的secondary region的数据。</p>
<p>明白了meta表中是如何存储secondary region数据，client要获取secondary region所在的RegionServer自然也简单，多解析几个server_xxxx的列便可以了。</p>
<p><img src="client-read-replicas.png" alt="client访问secondary region" title="client访问secondary region"></p>
<p>上图展示的是client访问secondary region的示意图。HBase的读请求有两种，Get和Scan。对于Get这种无状态的请求，每次RPC对server端来说都是一次独立的请求。client端的用户可以多次超时重试，直到获取到数据；也可以并发请求多个replica，选择率先返回的数据；还可以使用TIMELINE Read，请求primary region超时之后再请求其他secondary region。但对于Scan这种有状态的请求，一次scan可能与同一个region交互多次，也可能跨多个region多个RegionServer请求数据，server端会记录每个scan的状态数据，那么一次scan产生的多次RPC便不能随意地发给所有的replica。</p>
<p><img src="client_scan_replicas.png" alt="client scan过程" title="client scan过程"></p>
<p>上图展示的是client执行一个跨region的scan过程，假设当前表有2个逻辑region（Region_A和Region_B），region的起始区间分别为[a, d)、[d, f)，且该表的region replication为2，即每个逻辑region都有一主一备，4个region分布在4个RegionServer上。当我们执行一次scan操作，设置cacheing为2（每次RPC最多获取2个Result），则scan至少进行4次RPC，图中连线则表示每次RPC，连线上的数字表示RPC的顺序编号，虚线表示RPC超时或返回太慢结果没有被采用。可以看到当client要进行第1次RPC时，将请求同时发给了Region_A的主备2个region，因为此时server端是没有任何关于此次scan的状态数据，client可以选择率先返回响应的region进行后续的RPC交互。当第2次RPC时便不可以随意选择region了，因为Region_A_primary存储了此次scan的状态数据，而Region_A_replica_1没有，如果请求Region_A_replica_1则只会抛出异常。当第2次RPC结束，已经获取了Region_A中的全部数据，便可以清理掉Region_A_primary中存储的状态数据了。当第3次RPC时，和第1次时情况有些类似，server端暂时没有存储scan的状态数据了，client便可以像第1次RPC一样，将请求同时发给了Region_A的主备2个region。第4次RPC则像第2次一样。总结一下：当scan进行TIMELINE Read时，只有对每个逻辑region的第1次rpc可以任意选择region请求。</p>
<p>目前，Read Region Replicas功能并没有支持批量请求，即批量Get、Scan都是直接请求primary region。</p>
<h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>secondary region要支持读请求，则必然要有数据，而secondary region又不支持写请求，那么数据是哪来的呢？</p>
<p><img src="rs-structure.png" alt="RegionServer 内部结构" title="RegionServer 内部结构"></p>
<p>从HBase的数据模型上看，数据主要分为两部分：MemStore和HFile。HFile存储于HDFS上，secondary region只要及时获知HFile的变化便可以获取。但MemStore存在于内存，却只有primary region持有。以下便介绍两种secondary region同步数据的方式。</p>
<h4 id="StoreFile-Refresher"><a href="#StoreFile-Refresher" class="headerlink" title="StoreFile Refresher"></a>StoreFile Refresher</h4><p>第一种方式是StoreFile Refresher，在HBase-1.0+版本引入。在RegionServer上有一个StorefileRefresherChore任务，会定期地在HDFS上检查primary region的HFile有没有变化，以此来及时的发现primary region通过flush、compact、bulk load等操作产生的新HFile。</p>
<p>该方案实现上较为简单，也不需要太多多余的存储和网络开销，但缺点也非常明显，在数据写入primary region，到secondary region可以读到数据，有相当长的时间间隔，中间需要等待memstore的flush和StorefileRefresherChore任务的定时刷新。</p>
<p>如果要开启这个功能，只要将<code>hbase.regionserver.storefile.refresh.period</code>配置设置为非零值即可，表示StorefileRefresherChore任务刷新的时间间隔。</p>
<h4 id="Asynchronous-Replication"><a href="#Asynchronous-Replication" class="headerlink" title="Asynchronous Replication"></a>Asynchronous Replication</h4><p>HBase有提供集群间replication功能，利用WAL在多个集群之间同步数据。在HBase-1.1+版本中，便利用replication在集群内部同步数据，将实时写入的WAL同步到secondary region。</p>
<p><img src="region_replica_replication.png" alt="Asynchronous Replication示意图" title="Asynchronous Replication 示意图"></p>
<p>如上图中所示，通过实现一个特殊的<code>ReplicationEndpoint</code>便可以将WAL的数据同步给集群中的其他RegionServer。如此primary region MemStore中的数据，也通过replication实时同步到secondary region，从secondary region中也可以读到primary region还没有flush到HFile的数据。所以利用<code>Asnyc WAL replication</code>的同步方式比上面讲到的<code>StoreFile Refresher</code>同步方式具有更低的同步延迟。</p>
<p>primary region还会将flush、compaction和bulk load事件写到WAL，同样由replication功能同步到secondary region。当secondary region接收到这些事件时，便也回放同样的事件来更新自己的数据。所以对HFile文件列表的更新也比<code>StoreFile Refresher</code>定时刷新的方式更加实时。</p>
<p>在这种同步模式下，secondary region的MemStore中也是有数据，从WAL同步的Put/Delete操作就pPrimaryrRegion一样写入MemStore，并且secondary region也会使用block cache，所以在这种模式中内存的开销会成倍的增长。不同于primary region的是，secondary region在接收到flush事件时，并不会将MemStore中的数据flush成HFile，只会释放掉MemStore占用的内存。</p>
<p><code>Asnyc WAL replication</code>功能默认是关闭的。需要设置<code>hbase.region.replica.replication.enabled</code>为<code>true</code>来打开这个功能。当第一次创建一个region replication大于1的表时，将会创建一个名为<code>region_replica_replication</code>的replication peer，这个replication peer将负责集群内所有表region replica的数据同步。一旦开启之后想要再关闭该功能，就不只是改<code>hbase.region.replica.replication.enabled</code>为<code>false</code>了，还需要disable掉<code>region_replica_replication</code>这个replication peer。</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><h4 id="HFile的过期时间"><a href="#HFile的过期时间" class="headerlink" title="HFile的过期时间"></a>HFile的过期时间</h4><p>在以上两种数据同步方式中，都会在多个RegionServer上打开同一个HFile，所以当primary region进行完了major compaction之后，secondary region因为HFile文件变化更新不及时，依旧引用着旧的HFile。目前并没有有效的措施保证HFile文件并不会被过早的删除。只能是将配置项<code>hbase.master.hfilecleaner.ttl</code>设置为一个较大的值，比如一小时，以此来尽量避免请求过程中不会出错。但同时也会增加HDFS的存储开销。</p>
<h4 id="replication不能同步meta表数据"><a href="#replication不能同步meta表数据" class="headerlink" title="replication不能同步meta表数据"></a>replication不能同步meta表数据</h4><p>目前的Asynchronous Replication功能并不能同步meta表的WAL数据（最初该功能是用于集群间同步数据的，毕竟不能把meta数据同步给其他集群）。所以对于meta表的操作，并不能通过replication尽快的同步到secondary region，只能通过类似于<code>StoreFile Refresher</code>的方式，使用定时刷新的任务来同步meta表HFile文件的变化。</p>
<p><code>hbase.regionserver.meta.storefile.refresh.period</code>配置项用于控制meta表StoreFile的更新时间。该配置项并不同于<code>StoreFile Refresher</code>功能的<code>hbase.regionserver.storefile.refresh.period</code>。</p>
<h4 id="内存消耗"><a href="#内存消耗" class="headerlink" title="内存消耗"></a>内存消耗</h4><p>在之前已经提到，Asynchronous Replication同步因为使用MemStore和block cache，会导致内存开销成倍增加。并且secondary region并不会主动进行flush，只会当接收到同步的WAL中的flush事件时，才会进行flush。在一些极端情况下，比如replication阻塞收不到flush事件、primary region确实长时间没有进行flushsecondaryarRegion持有的内存得不到释放，而一个RegionServer上同时有多个primary region和secondary region，内存的过度消耗可能会阻塞primary region正常的写入操作，也会阻塞replication同步的flush事件。</p>
<p>所以HBase提供了一个配置项<code>hbase.region.replica.storefile.refresh.memstore.multiplier</code>，默认值为4，表示如果secondary region的MemStore比primary region最大的MemStore的4倍还要大时，便允secondaryarRegion自行refresh检查HFile文件是否变化，如果primary region早已flush过，却因为replication阻塞没有同步到，则可以利用该机制进行flush。默认情况下最好不要执行这个操作，可以把该配置项设置大一些来避免。</p>
<h4 id="secondary-region-Failover"><a href="#secondary-region-Failover" class="headerlink" title="secondary region Failover"></a>secondary region Failover</h4><p>当一个secondary region刚open或者fail over，此时必然丢失了之前MemStore的数据，因为secondary region毕竟不能像primary region一样通过回放WAL来恢复MemStore。如果此时直接提供读服务，则可能出现数据版本回退的问题，即恢复之后比恢复之前读到的数据更旧。为了避免数据回退，secondary region就必须等待primary region进行一次完整的flush操作或open region事件，在这之前，secondary region都将拒绝接服务。</p>
<p><code>hbase.region.replica.wait.for.primary.flush</code>配置项是该机制的开关，默认是<code>enable</code>开启。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p><strong>server端</strong></p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>默认值</th>
<th>单位</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>hbase.regionserver.storefile.refresh.period</td>
<td>0</td>
<td>毫秒</td>
<td>secondary region刷新storefile的时间间隔，默认0为关闭</td>
</tr>
<tr>
<td>hbase.regionserver.meta.storefile.refresh.period</td>
<td>0</td>
<td>毫秒</td>
<td>secondary region刷新hbase:meta表storefile的时间间隔，默认0为关闭</td>
</tr>
<tr>
<td>hbase.region.replica.replication.enabled</td>
<td>false</td>
<td></td>
<td>是否开启<code>Asnyc WAL replication</code>功能，开启后再想关闭，需要改为false之后再disable掉<code>region_replica_replication</code>的peer</td>
</tr>
<tr>
<td>hbase.master.hfilecleaner.ttl</td>
<td>300000(5分钟)</td>
<td>毫秒</td>
<td>storefile文件的过期删除时间间隔</td>
</tr>
<tr>
<td>hbase.meta.replica.count</td>
<td>1</td>
<td>个</td>
<td>meta表的region replication数量</td>
</tr>
<tr>
<td>hbase.region.replica.storefile.refresh.memstore.multiplier</td>
<td>4</td>
<td>倍</td>
<td>secondary region的MemStore大于同RegionServer上primary region最大的MemStore该倍数时，会触发刷新storefile文件列表的任务</td>
</tr>
<tr>
<td>hbase.region.replica.wait.for.primary.flush</td>
<td>true</td>
<td></td>
<td>secondary region open之后，是否要等待primary region进行一次flush再提供服务</td>
</tr>
<tr>
<td>hbase.master.loadbalancer.class</td>
<td>org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer</td>
<td></td>
<td>默认的实现可以保证region的replicas尽量不会分布在同一个RegionServer上，如果修改该配置，要注意replicas的分布</td>
</tr>
</tbody></table>
<p><strong>client端</strong></p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>默认值</th>
<th>单位</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>hbase.ipc.client.specificThreadForWriting</td>
<td>false</td>
<td></td>
<td>是否使用特殊线程用于写请求。使用region replicas功能，经常会在IO过程中中断线程，所以必须开启该配置</td>
</tr>
<tr>
<td>hbase.client.primaryCallTimeout.get</td>
<td>10000</td>
<td>微秒</td>
<td>TIMELINE一致性Get时，等待primary region响应的时间，超时之后便请求secondary region</td>
</tr>
<tr>
<td>hbase.client.primaryCallTimeout.scan</td>
<td>10000</td>
<td>微秒</td>
<td>TIMELINE一致性Scan时，等待primary region响应的时间，超时之后便请求secondary region</td>
</tr>
<tr>
<td>hbase.meta.replicas.use</td>
<td>false</td>
<td></td>
<td>是否使用meta表的secondary region</td>
</tr>
</tbody></table>
<h3 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h3><p><code>REGION_REPLICATION</code>参数控制表中region有多少备份，默认值为1，即只有primary region。</p>
<p>shell方式建表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create 't1', 'f1', &#123;REGION_REPLICATION =&gt; 2&#125;</span><br></pre></td></tr></table></figure>
<p>shell方式修改表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter 't1', &#123;REGION_REPLICATION =&gt; 2&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>Client访问secondary region必须要用户明确的表示可以接收非强一致性的数据，如果希望请求可以发送给secondary region，必须明确指定为<code>TIMELINE</code>的一致性。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> Consistency &#123;</span><br><span class="line">    STRONG,</span><br><span class="line">    TIMELINE</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h4><p>允许以<code>TIMELINE</code>的一致性读取数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; get 't1','r6', &#123;CONSISTENCY =&gt; "TIMELINE"&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; get 't1','r6', &#123;CONSISTENCY =&gt; "TIMELINE", , REGION_REPLICA_ID =&gt; 1&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h4><p>Get</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Get get = <span class="keyword">new</span> Get(row);</span><br><span class="line">get.setConsistency(Consistency.TIMELINE);</span><br><span class="line">...</span><br><span class="line">Result result = table.get(get);</span><br></pre></td></tr></table></figure>
<p>Scan</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setConsistency(Consistency.TIMELINE);</span><br><span class="line">...</span><br><span class="line">ResultScanner scanner = table.getScanner(scan);</span><br></pre></td></tr></table></figure>

<p>可以通过<code>Result.isStale()</code>判断数据是否来自于secondary region</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">if</span> (result.isStale()) &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p><strong>机器配置</strong></p>
<p>HBase版本：2.2.0<br>HDFS版本： 3.1.4</p>
<table>
<thead>
<tr>
<th>service</th>
<th>job</th>
<th>实例数</th>
<th>cpu</th>
<th>disk</th>
<th>netowork</th>
<th>comment</th>
</tr>
</thead>
<tbody><tr>
<td>HBase</td>
<td>master</td>
<td>2</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>HBase</td>
<td>region server</td>
<td>5</td>
<td>24 core</td>
<td>12*3.7T HDD</td>
<td>10000bps</td>
<td>onheap=50g/offheap=50g</td>
</tr>
<tr>
<td>HDFS</td>
<td>namenode</td>
<td>2</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>onheap=10g</td>
</tr>
<tr>
<td>HDFS</td>
<td>datanode</td>
<td>5</td>
<td>24 core</td>
<td>12*3.7T HDD</td>
<td>10000bps</td>
<td>onheap=2g</td>
</tr>
</tbody></table>
<p><strong>不限制QPS</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>strong</th>
<th>timeline-10ms</th>
<th>reta</th>
</tr>
</thead>
<tbody><tr>
<td>qps_sec</td>
<td>12608.23</td>
<td>11171.18</td>
<td>-11.4%</td>
</tr>
<tr>
<td>max_latency_us</td>
<td>195174.38</td>
<td>202603.69</td>
<td>3.81%</td>
</tr>
<tr>
<td>min_latency_us</td>
<td>149.38</td>
<td>148.75</td>
<td>-0.42%</td>
</tr>
<tr>
<td>avg_latency_us</td>
<td>3760.69</td>
<td>4276.76</td>
<td>13.72%</td>
</tr>
<tr>
<td>p90_latency_us</td>
<td>11811.92</td>
<td>14258.31</td>
<td>20.71%</td>
</tr>
<tr>
<td>p99_latency_us</td>
<td>32512.23</td>
<td>31148.14</td>
<td>-4.2%</td>
</tr>
<tr>
<td>p999_latency_us</td>
<td>64646.38</td>
<td>58621.93</td>
<td>-9.32%</td>
</tr>
<tr>
<td>p9999_latency_us</td>
<td>136835.92</td>
<td>115951.63</td>
<td>-15.26%</td>
</tr>
</tbody></table>
<p><strong>限制7000QPS</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>strong</th>
<th>timeline-10ms</th>
<th>reta</th>
</tr>
</thead>
<tbody><tr>
<td>qps_sec</td>
<td>6999.58</td>
<td>6999.56</td>
<td>-0.0%</td>
</tr>
<tr>
<td>max_latency_us</td>
<td>126860.75</td>
<td>130148.86</td>
<td>2.59%</td>
</tr>
<tr>
<td>min_latency_us</td>
<td>147.25</td>
<td>150.68</td>
<td>2.33%</td>
</tr>
<tr>
<td>avg_latency_us</td>
<td>3223.38</td>
<td>3495.51</td>
<td>8.44%</td>
</tr>
<tr>
<td>p90_latency_us</td>
<td>10612.49</td>
<td>11379.48</td>
<td>7.23%</td>
</tr>
<tr>
<td>p99_latency_us</td>
<td>23793.54</td>
<td>24469.25</td>
<td>2.84%</td>
</tr>
<tr>
<td>p999_latency_us</td>
<td>48791.00</td>
<td>39795.06</td>
<td>-18.44%</td>
</tr>
<tr>
<td>p9999_latency_us</td>
<td>93389.08</td>
<td>78618.61</td>
<td>-15.82%</td>
</tr>
</tbody></table>
<p>对单Client实例做压力测试，<code>hbase.client.primaryCallTimeout.get</code>参数设置为10000，即等待primary region响应的时间超时10ms之后便请求secondary region。</p>
<p>第一组极限QPS的压测中，可以看出开启TIMELINE Read之后，QPS有一定损失，平均延迟有一定升高，P999和P9999一定程度优化。优化效果有限。</p>
<p>因为read replicas会增加线程资源的使用，而日常使用也不会把Client侧压到极限，所以又做了一组限制QPS的压测，可以看到各项延迟指标均有所好转。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>Region Replica</code>功能可以提高HBase的读可用性，但也要根据具体的用例考虑是否适用。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>当应用依赖是只读的表，或者应用并不要求强一致性（要求最终一致性，可以接受短时间内数据不一致）时，可以使用该功能来提高读可用性。在RegionServer或Region出现单点故障恢复期间或长时间Full GC期间尽量保证业务读请求正常，减少MTTR过长对业务产生的影响，同时也可以减少大量重试请求进一步地增加故障节点的压力。</li>
<li>对于部分不要求强一致性且对延迟毛刺有一定要求的应用，当在Client侧QPS较低或CPU、带宽等资源富余时，可以使用该功能降低读请求P99/P999和P9999延迟。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>多倍的MemStore导致更多的内存消耗</li>
<li>增加block cache的需求和使用</li>
<li>为了传输WAL导致更多的网络带宽消耗</li>
<li>大量的集群内部RPC请求</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://hbase.apache.org/book.html#arch.timelineconsistent.reads" target="_blank" rel="noopener">Timeline-consistent High Available Reads</a></li>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10070" target="_blank" rel="noopener">HBASE-10070</a></li>
<li><a href="https://issues.apache.org/jira/secure/attachment/12616659/HighAvailabilityDesignforreadsApachedoc.pdf" target="_blank" rel="noopener">HighAvailabilityDesignforreadsApachedoc.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Region Replicas</tag>
      </tags>
  </entry>
  <entry>
    <title>「记」Jdk8 CompletableFuture在高并发环境下的性能问题</title>
    <url>/blog/2019/10/11/CompletableFuture-in-high-concurrent/</url>
    <content><![CDATA[<p>最近的工作内容就是写一个 DualHBaseClient，在查询数据时间过长时，能够将同样的请求发给 replication 的集群，缩小 client 端的 p99、p999 延迟，减小毛刺。<br>实际开发最初的一版代码都没有花费1pd，性能测试倒测了好几天都不及预期，甚至优化之后各方面性能更差劲。</p>
<p>本文就是记录下导致此次性能问题的主要原因：CompletableFuture.</p>
<p>使用 Java 异步编程的时候，CompletableFuture 用起来还是相当舒服的，在HBase的异步API里，也大量的使用了CompletableFuture，如果 CompletableFuture 有性能问题，那可就悲催了。</p>
<a id="more"></a>

<p>看下以下这段测试CompletableFuture的代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import com.google.common.collect.Lists;</span><br><span class="line"></span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.concurrent.CompletableFuture;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class FutureTest &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        new FutureTest().run();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void run() throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        List&lt;CompletableFuture&gt; futures &#x3D; Lists.newArrayList();</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 1000; i++) &#123;</span><br><span class="line">            CompletableFuture future &#x3D; new CompletableFuture();</span><br><span class="line">            CompletableFuture f &#x3D; CompletableFuture.runAsync(this::read);</span><br><span class="line">            f.whenComplete((r, e) -&gt; future.complete(r));</span><br><span class="line">            futures.add(future);</span><br><span class="line">        &#125;</span><br><span class="line">        for (CompletableFuture future : futures) &#123;</span><br><span class="line">            future.get();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void read() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Thread.sleep(100);</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上这段代码实际只是让 CompletableFuture 空转，除了 sleep 没再做其他的，循环1000让运行时间尽量长一些，足够让我们跑一个火焰图出来。</p>
<p><img src="CompletableFuture-in-jdk8-traces.svg" alt="火焰图"></p>
<p>火焰图里注意到有个最大的平顶 java.lang.Runtime.availableProcessors，该方法耗时极大，甚至超过了 Thread.sleep，这可不正常吧。</p>
<p>随后我们测下 java.lang.Runtime.availableProcessors() 方法是不是真的慢。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import com.google.common.base.Stopwatch;</span><br><span class="line">import org.junit.Assert;</span><br><span class="line">import org.junit.Test;</span><br><span class="line"></span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class AvailableProcessorsTest &#123;</span><br><span class="line"></span><br><span class="line">    @Test</span><br><span class="line">    public void test() &#123;</span><br><span class="line">        Stopwatch sw &#x3D; Stopwatch.createStarted();</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 1000000; i++) &#123;</span><br><span class="line">            Runtime.getRuntime().availableProcessors();</span><br><span class="line">        &#125;</span><br><span class="line">        Assert.assertTrue(sw.elapsed(TimeUnit.SECONDS) &gt; 10);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在我的机器上循环1000000次，耗时超过了10s，不算快。</p>
<p>再来看看 CompletableFuture 是怎么使用 Runtime.availableProcessors() 的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private Object waitingGet(boolean interruptible) &#123;</span><br><span class="line">    ...</span><br><span class="line">    while ((r &#x3D; result) &#x3D;&#x3D; null) &#123;</span><br><span class="line">        if (spins &lt; 0)</span><br><span class="line">            spins &#x3D; (Runtime.getRuntime().availableProcessors() &gt; 1) ?</span><br><span class="line">                1 &lt;&lt; 8 : 0; &#x2F;&#x2F; Use brief spin-wait on multiprocessors</span><br><span class="line">        else if (spins &gt; 0) &#123;</span><br><span class="line">            if (ThreadLocalRandom.nextSecondarySeed() &gt;&#x3D; 0)</span><br><span class="line">                --spins;</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>while 死循环要直到出现异常或获取到最终结果才会结束，而循环中又会大量调用 Runtime.availableProcessors()，这就是 CompletableFuture 存在的性能问题。</p>
<p>而实际上Openjdk已有对该问题的讨论，<a href="https://bugs.openjdk.java.net/browse/JDK-8227018" target="_blank" rel="noopener">JDK-8227018</a>，该优化也很简单，在这里其实只是需要知道运行环境是不是多处理器环境而已，缓存起来就好，完全没必要每次循环都去获取。在jdk8u232版本就fix掉了。</p>
<p>而且我还看了 jdk11、jdk13 的实现，都不再使用 Runtime.availableProcessors() 了，所以算是低版本 jdk8 用户独有的烦恼。</p>
<blockquote>
<p>一个小插曲</p>
<p>jdk8u232版本是2019.10.15才正式release的，而我发现这个问题是在10.13。<br>也幸运也不幸，幸运的是起码问题fix掉了，不幸的是 DualHBaseClient 不能采用 CompletableFuture 实现了，总不好要求用户升级jdk吧</p>
</blockquote>
<p>知道低版本的 jdk8 有问题之后，DualHBaseClient 还是要写的，只能寻求其他的异步框架来实现，好在 guava 的 ListenableFuture 实现很像 CompletableFuture。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果是jdk8低版本用户（刚发布几天不会有公司升级吧）频繁的大量使用 CompletableFuture，是存在性能问题，升级 jdk 是最简单的办法，使用 guava 的 Future 库实现也可以，但可能要大量修改代码了。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul>
<li><a href="https://blog.wangqi.love/articles/Java/%E7%81%AB%E7%84%B0%E5%9B%BE%E6%8E%92%E6%9F%A5Java%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98.html" target="_blank" rel="noopener">火焰图排查Java性能问题</a></li>
<li><a href="https://bugs.openjdk.java.net/browse/JDK-8227018" target="_blank" rel="noopener">JDK-8227018</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Jdk</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>「记」子类复写父类方法与类初始化顺序引发的bug</title>
    <url>/blog/2019/09/30/class-init-order-in-inheritance/</url>
    <content><![CDATA[<h2 id="bug现象"><a href="#bug现象" class="headerlink" title="bug现象"></a>bug现象</h2><p>bug出现的条件：</p>
<ul>
<li>继承关系</li>
<li>子类属性有默认的初始化</li>
<li>子类复写了父类的钩子方法</li>
<li>钩子方法在父类构造方法中调用</li>
</ul>
<a id="more"></a>

<p>可以看下以下的示例代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Father &#123;</span><br><span class="line"></span><br><span class="line">    public Father() &#123;</span><br><span class="line">        init();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    protected void init() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Child extends Father &#123;</span><br><span class="line"></span><br><span class="line">    int a &#x3D; 0;</span><br><span class="line"></span><br><span class="line">    public Child() &#123;</span><br><span class="line">        super();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void init() &#123;</span><br><span class="line">        super.init();</span><br><span class="line">        a &#x3D; 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.junit.Test;</span><br><span class="line"></span><br><span class="line">import static org.junit.Assert.*;</span><br><span class="line"></span><br><span class="line">public class ChildTest &#123;</span><br><span class="line"></span><br><span class="line">    @Test</span><br><span class="line">    public void testConstructor() &#123;</span><br><span class="line">        Child child &#x3D; new Child();</span><br><span class="line">        assertEquals(0, child.a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="类加载过程"><a href="#类加载过程" class="headerlink" title="类加载过程"></a>类加载过程</h2><p>顺便复习一下类加载过程，类加载过程分为加载、链接、初始化三大步骤</p>
<p>加载：查找并加载类的二进制字节流数据，并且据此创建类，即代表这个类的Class对象。</p>
<p>链接：将创建成的类合并至Java虚拟机中，使之能够执行的过程。还分验证、准备、解析三个阶段。</p>
<ul>
<li>验证：确保被加载类的正确性</li>
<li>准备：为类的静态变量分配内存，并将其初始化为默认值</li>
<li>解析：把类中的符号引用转换为直接引用</li>
</ul>
<p>初始化：标记为常量值的字段赋值，以及执行方法。初始化的时机：</p>
<ol>
<li>虚拟机启动时，初始化用户指定的主类</li>
<li>当遇到以新建目标类实例的new指令时，初始化new指定的目标类</li>
<li>当遇到调用静态方法的指令字段是，初始化该静态方法所在的类</li>
<li>当遇到访问静态字段的指令时，初始化该静态字段所在的类</li>
<li>子类的初始化会触发父类的初始化</li>
<li>如果一个接口定义了default方法，那么直接或间接实现该接口的类初始化，会触</li>
<li>该接口的初始化</li>
<li>使用反射API对某个类进行反射调用时，初始化这个类</li>
<li>当初次调用MethodHandle实例时，初始化该MethodHandle指向的方法所在的类</li>
</ol>
<h2 id="类初始化顺序"><a href="#类初始化顺序" class="headerlink" title="类初始化顺序"></a>类初始化顺序</h2><ol>
<li>按定义顺序初始化父类的static成员，在准备阶段完成</li>
<li>按定义顺序初始化子类的static成员，在准备阶段完成</li>
<li>按定义顺序执行父类的普通成员初始化</li>
<li>执行父类的构造函数</li>
<li>按定义顺序执行子类的指定初始化</li>
<li>执行子类的构造函数</li>
</ol>
<h2 id="回头解bug"><a href="#回头解bug" class="headerlink" title="回头解bug"></a>回头解bug</h2><p>现在再回头看下bug代码就知道是什么原因：父类执行构造方法时调用了会修改子类成员<code>a</code>的钩子方法，然后子类才初始化了自己的成员，覆盖了钩子方法里对<code>a</code>的修改。</p>
<p>解决方法也简单，对<code>a</code>不加默认值就好了。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>bug</tag>
      </tags>
  </entry>
</search>
